---
---

@string{aps = {American Physical Society,}}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation,},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics,},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers,}
}


@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.,},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  abstract={Recent years have witnessed the emergence of a new class of cooperative edge systems in which a large number of edge nodes can collaborate through local peer-to-peer connectivity. 
  In this paper, we propose CoEdge, a novel cooperative edge system that can support concurrent data/compute-intensive deep learning (DL) models for distributed real-time applications such as city-scale traffic monitoring and autonomous driving. 
  First, CoEdge includes a hierarchical DL task scheduling framework that dispatches DL tasks to edge nodes based on their computational profiles, communication overhead, and real-time requirements. 
  Second, CoEdge can dramatically increase the execution efficiency of DL models by batching sensor data and aggregating the inferences of the same model.
  Finally, we propose a new edge containerization approach that enables an edge node to execute concurrent DL tasks by partitioning the CPU and GPU workloads into different containers.
  We extensively evaluate CoEdge on a self-deployed smart lamppost testbed on a university campus. Our results show that CoEdge can achieve up to 82.32% reduction on deadline missing rate compared to baselines.},

}

@article{ouyang2023Harmony,
  abbr={MobiSys'23},
  title={Harmony: Heterogeneous Multi-Modal Federated Learning through Disentangled Model Training},
  author={Ouyang, Xiaomin and Xie, Zhiyuan and Fu, Heming and Pan, Li and Chen, Sitong and Ling, Neiwen and Xing, Guoliang and Zhou, Jiayu and Huang, Jianwei},
  journal = {The 21st ACM International Conference on Mobile Systems, Applications, and Services (ACM MobiSys 2023)},
  year={2023},
  abstract={Multi-modal sensing systems are increasingly prevalent in realworld applications such as health monitoring and autonomous driving. Most multi-modal learning approaches need to access users’ raw data, which poses significant concerns to users’ privacy. Federated learning (FL) provides a privacy-aware distributed learning framework. 
  However, current FL approaches have not addressed the unique challenges of heterogeneous multi-modal FL systems, such as modality heterogeneity and significantly longer training delay. 
  In this paper, we propose Harmony, a new system for heterogeneous multi-modal federated learning. Harmony disentangles the multimodal network training in a novel two-stage framework, namely modality-wise federated learning and federated fusion learning. 
  By integrating a novel balance-aware resource allocation mechanism in modality-wise FL and exploiting modality biases in federated fusion learning, Harmony improves the model accuracy under noni.i.d. data distributions and speeds up system convergence. 
  We implemented Harmony on a real-world multi-modal sensor testbed deployed in the homes of 16 elderly subjects for Alzheimer’s Disease monitoring. 
  Our evaluation on the testbed and three large-scale public datasets of different applications show that, Harmony outperforms by up to 46.35% accuracy over state-of-the-art baselines
  and saves up to 30% training delay.},
  accept={(Acceptance ratio: 41/198=20.7%)}
}

@article{jiang2023coedge,
  abbr={IPSN'23},
  selected={true},
  title={CoEdge: A Cooperative Edge System for Distributed Real-Time Deep Learning Tasks},
  author={Jiang(co-primary), Zhehao and Ling(co-primary), Neiwen and Huang, Xuan and Shi, Shuyao and Wu, Chenhao and Zhao, Xiaoguang and Yan, Zhenyu and Xing, Guoliang},
  journal = {The 22nd ACM/IEEE Conference on Information Processing in Sensor Networks (ACM/IEEE IPSN 2023)},
  year={2023},
  pdf={coedge_ipsn23.pdf},
  accept={(Acceptance ratio: 22/83=26.51%)},
  html={https://dl.acm.org/doi/10.1145/3583120.3586955}
}


@article{xie2023timely,
  abbr={DATE'23},
  title={Timely Fusion of Surround Radar/Lidar for Object Detection in Autonomous Driving Systems},
  author={Xie, Wenjing and Hu, Tao and Ling, Neiwen and Xing, Guoliang and Liu, Shaoshan and Guan, Nan},
  journal = {Design, Automation & Test in Europe Conference & Exhibition (IEEE DATE 2023)},
  year={2023},
  abstract={Fusion of multiple sensor modalities, such as camera, Lidar and Radar, are commonly used in autonomous driving systems to fully utilize the complementary advantages of different sensors. 
  Surround Radar/Lidar can provide 360-degree view sampling with the minimal cost, which are promising sensing hardware solutions for autonomous driving systems. However, due to the intrinsic physical constraints, the rotating speed (i.e., the frequency to generate data frames) of surround Radar is much lower than surround Lidar, and existing Radar/Lidar fusion methods have to work at the low frequency of surround Radar, which cannot meet the high responsiveness requirement of autonomous driving systems. 
  This paper develops techniques to fuse surround Radar/Lidar with working frequency only limited by the faster surround Lidar instead of the slower surround Radar, based on the state-of-the-art Radar/Lidar DNN model MVDNet. 
  The basic idea of our approach is simple: we let MVDNet work with temporally unaligned data from Radar/Lidar, so that fusion can take place at any time when a new Lidar data frame arrives, instead of waiting for the slow Radar data frame. 
  However, directly applying MVDNet to temporally unaligned Radar/Lidar data greatly degrades its object detection accuracy. 
  The key information revealed in this paper is that we can achieve high output frequency with little accuracy loss by enhancing the training procedure to explore the temporal redundancy in fusion procedure of MVDNet so that it can tolerate the temporal unalignment of the input data. 
  We explore several different ways of training enhancement and compare them quantitatively with experiments.}
}

@article{zhao2023moses,
  abbr={HotMobile'23},
  title={Moses: Exploiting Cross-device Transferable Features for On-device Tensor Program Optimization},
  author={Zhao, Zhihe and Shuai, Xian and Ling, Neiwen and Guan, Nan and Yan, Zhenyu and Xing, Guoliang},
  journal = {The 24th International Workshop on Mobile Computing Systems and Applications (ACM HotMobile 2023)},
  year={2023},
  abstract={Achieving efficient execution of machine learning models on mobile/edge devices has attracted significant attention recently. 
  A key challenge is to generate high-performance tensor programs for each operator inside a DNN model efficiently. To this end, deep learning compilers have adopted auto-tuning approaches such as Ansor. 
  However, it is challenging to optimize tensor codes for mobile/edge devices by auto-tuning due to limited time budgets and on-device resources. 
  A key component of DNN compilers is the cost model that can predict the performance of each configuration on specific devices. 
  However, current design of cost models cannot provide transferable features among different hardware accelerators efficiently and effectively. 
  In this paper, we propose Moses, a simple yet efficient design based on the lottery ticket hypothesis, which fully takes advantage of the hardware-agnostic features transferable to the target device via domain adaptation to optimize the time-consuming auto-tuning process of DNN compiling on a new hardware platform. 
  Compared with state-of-the-art approaches, Moses achieves up to 1.53X efficiency gain in the search stage and 1.41X inference speedup on challenging DNN benchmarks.},
  pdf={moses-hotmobile.pdf},
  html={https://dl.acm.org/doi/abs/10.1145/3572864.3580330}
}

@article{ling2022blastnet,
  abbr={SenSys'22},
  selected={true},
  title={BlastNet: Exploiting Duo-Blocks for Cross-Processor Real-Time DNN Inference},
  author={Ling, Neiwen and Huang, Xuan and Zhao, Zhihe and Guan, Nan and Yan, Zhenyu and Xing, Guoliang},
  journal={Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems (ACM SenSys 2022)},
  year={2022},
  award={Best Paper Finalist},
  accept={(Acceptance ratio: 52/208=25%)},
  abstract = {In recent years, Deep Neural Network (DNN) has been increasingly adopted by a wide range of time-critical applications running on 
  edge platforms with heterogeneous multiprocessors. To meet the stringent timing requirements of these applications, heterogeneous 
  CPU and GPU resources must be efficiently utilized for the inference of multiple DNN models. Such a cross-processor real-time 
  DNN inference paradigm poses major challenges due to the inherent performance imbalance among different processors and the lack 
  of real-time support for cross-processor inference from existing deep learning frameworks. In this work, we propose a new system 
  named BlastNet that exploits duo-block - a new model inference abstraction to support highly efficient cross-processor real-time DNN 
  inference. Each duo-block has a dual model structure, enabling efficient fine-grained inference alternatively across different processors. BlastNet employs a novel block-level Neural Architecture 
  Search (NAS) technique to generate duo-blocks, which accounts for computing characteristics and communication overhead. The duoblocks are optimized at design time and then dynamically scheduled 
  to achieve high resource utilization of heterogeneous CPU and GPU at runtime. BlastNet is implemented on an indoor autonomous driving platform and three popular edge platforms. Extensive results 
  show that BlastNet achieves 35.07 % less deadline missing rate with a mere 1.63% of model accuracy loss.},
  pdf = {blastnet_sensys2022.pdf},
  html={https://dl.acm.org/doi/abs/10.1145/3560905.3568520},
  slides={Blastnet-slides.pdf}
}


@article{zhao2022iot,
  abbr={SenSys'22Poster},
  selected={true},
  title={Aaron: Compile-time Kernel Adaptation for Multi-DNN Inference Acceleration on Edge GPU},
  author={Zhao, Zhihe and Ling, Neiwen and Guan, Nan and Xing, Guoliang},
  journal={Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
  year={2022},
  award={Best Poster Award},
  abstract={AI applications powered by deep learning are increasingly running on edge devices. Meanwhile, many real-world IoT applications demand multiple real-time tasks to run on the same device, for
  example, to achieve both object tracking and image segmentation simultaneously on an augmented reality glass. However, the current solutions can not yet support such multi-tenant real-time DNN inference 
  on edge devices. Techniques such as on-device model compression trade inference accuracy for speed, while traditional DNN compilers mainly focus on single-tenant DNN model optimization. 
  To fill this gap, we propose Aaron, which leverages DNN compiling techniques to accelerate multi-DNN inference on edge GPU based on compile-time kernel adaptation with no accuracy loss. 
  Aaron integrates both DNN graph and kernel optimization to maximize on-device parallelism and minimize contention brought by concurrent inference.},
  pdf={aaron_sensys2022.pdf}
}

@article{ling2021rt,
  abbr={SenSys'21},
  selected={true},
  title={RT-mDL: Supporting Real-Time Mixed Deep Learning Tasks on Edge Platforms},
  author={Ling, Neiwen and Wang, Kai and He, Yuze and Xing, Guoliang and Xie, Daqi},
  abstract={Recent years have witnessed an emerging class of real-time applications, e.g., autonomous driving, in which resource-constrained edge platforms need to execute a set of real-time mixed Deep Learning (DL) tasks concurrently. Such an application paradigm poses major challenges due to the huge compute workload of deep neural network models, diverse performance requirements of different tasks, and the lack of real-time support from existing DL frameworks. In this paper, we present RT-mDL, a novel framework to support mixed real-time DL tasks on edge platform with heterogeneous CPU and GPU resource. 
  RT-mDL aims to optimize the mixed DL task execution to meet their diverse real-time/accuracy requirements by exploiting unique compute characteristics of DL tasks. RT-mDL employs a novel storage-bounded model scaling method to generate a series of model variants, and systematically optimizes the DL task execution by joint model variants selection and task priority assignment. To improve the CPU/GPU utilization of mixed DL tasks, RT-mDL also includes a new priority-based scheduler which employs a GPU packing mechanism and executes the CPU/GPU tasks independently.
  Our implementation on an F1/10 autonomous driving testbed shows that, RT-mDL can enable multiple concurrent DL tasks to achieve satisfactory real-time performance in traffic light detection and sign recognition. Moreover, compared to state-of-the-art baselines, RT-mDL can reduce deadline missing rate by 40.12\% while only sacrificing 1.7\% model accuracy. 
  },
  journal={Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems (ACM SenSys 2021)},
  pages={1--14},
  year={2021},
  doi={10.1145/3485730.3485938},
  url={https://dl.acm.org/doi/pdf/10.1145/3485730.3485938},
  html={https://dl.acm.org/doi/abs/10.1145/3485730.3485938},
  pdf={RT-mDL_SenSys2021.pdf},
  accept={(Acceptance ratio: 25/139=17.98%)},
  video={https://www.youtube.com/watch?v=9Hh1NOyXqPU&t=8s},
  demo={https://www.youtube.com/watch?v=7-_t1MrIo04},
  slides={RT-mDL-slides.pdf}
}

@article{zhao2018ecrt,
  abbr={SenSys'18Demo},
  title={ECRT: An edge computing system for real-time image-based object tracking},
  author={Zhao, Zhihe and Jiang, Zhehao and Ling, Neiwen and Shuai, Xian and Xing, Guoliang},
  journal={Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems},
  pages={394--395},
  year={2018},
  abstract = {Real-time image-based object tracking from live video is of great importance for several smart city applications like surveillance, intelligent traffic management and autonomous driving. 
  Although recent deep learning systems can achieve satisfactory tracking performance, they incur significant compute overhead, which prevents them from wide adoption on resource-constrained IoT platforms. 
  In this demonstration, we present an Edge Computing system for Real-time object Tracking (ECRT) for resource-constrained devices. 
  The key feature of our system is that it intelligently partitions compute-intensive tasks such as inferencing a convolutional neural network(CNN) into two parts, which are executed locally on an IoT device and/or on the edge server. 
  Moreover, ECRT can minimize the power consumption of IoT devices while taking into consideration the dynamic network environment and user requirement on end to end delay.},
  doi={10.1145/3274783.3275199},
  url={https://dl.acm.org/doi/pdf/10.1145/3274783.3275199},
  html={https://dl.acm.org/doi/abs/10.1145/3274783.3275199},
  pdf={ECRT.pdf},
}

@article{zhao2021edgeml,
  abbr={IoTDI'21},
  selected={true},
  title={EdgeML: An AutoML framework for real-time deep learning on the edge},
  author={Zhao, Zhihe and Wang, Kai and Ling, Neiwen and Xing, Guoliang},
  abstract = {In recent years, deep learning algorithms are increasingly adopted by a wide range of data-intensive and time-critical Internet of Things (IoT) applications. 
  As a result, several new approaches, including model partition/offloading and progressive neural architecture, have been proposed to address the challenge of deploying the computation-intensive deep neural network (DNN) models on resource-constrained edge devices. 
  However, the performance of existing approaches is highly affected by runtime dynamics. For example, offloading workload from edge to cloud suffers from communication delays and the efficiency of progressive neural architecture supporting early-exit DNN executions relies on input characteristics. 
  In this paper, we introduce EdgeML, an AutoML framework that provides flexible and fine-grained DNN model execution control by combining workload offloading mechanism and dynamic progressive neural architecture. 
  To achieve desirable latency-accuracy-energy system performance on edge platforms, EdgeML adopts reinforcement learning to automatically update model execution policy in response to runtime dynamics in real-time. We implement EdgeML for several widely used DNN models on the latest edge devices. 
  Comparing to existing approaches, our experiments show that EdgeML achieves up to 8x performance improvement under dynamic environments.},
  journal={Proceedings of the 6th International Conference on Internet-of-Things Design and Implementation (ACM/IEEE IoTDI 2021)},
  pages={133--144},
  year={2021},
  doi={10.1145/3450268.3453520},
  url={https://dl.acm.org/doi/pdf/10.1145/3450268.3453520},
  html={https://dl.acm.org/doi/abs/10.1145/3450268.3453520},
  pdf={EdgeML_IoTDI2021.pdf},
  accept={(Acceptance ratio: 19/74=25.7%)}
}

@article{ling2022dataset,
  abbr={SenSys'22Workshop},
  title={Dataset: An Indoor Smart Traffic Dataset and Data Collection System},
  author={Ling(co-primary), Neiwen and He(co-primary), Yuze and Guan, Nan and Fu, Heming and Xing, Guoliang},
  journal = {The 5th International SenSys/BuildSys Workshop on Data},
  year={2022},
  abstract={Smart traffic is an emerging research area gaining more attention due to a class of emerging applications such as autonomous driving. 
  Most smart traffic scenarios are outdoors, which are hard to collect traffic data and build demanding sensing systems. In this work, 
  an indoor smart traffic testbed with an F1TENTH autonomous driving vehicle is built, allowing the collection of traffic datasets 
  under different scenarios and performing various smart traffic tasks. This novel data collection system and collected dataset can help 
  research teams build various smart traffic systems and evaluate indoor smart traffic datasets. The collected traffic light dataset is 
  publicly available at the link1.},
  pdf={data22.pdf},
  html={https://dl.acm.org/doi/abs/10.1145/3560905.3567772},
  dataset={{https://zenodo.org/record/7181314#%23.Y0a0qXZBxD8}}
}

@article{zhao2022moses,
  abbr ={arXiv},
  title={Moses: Efficient Exploitation of Cross-device Transferable Features for Tensor Program Optimization},
  author={Zhao, Zhihe and Shuai, Xian and Bai, Yang and Ling, Neiwen and Guan, Nan and Yan, Zhenyu and Xing, Guoliang},
  abstract = {Achieving efficient execution of machine learning models has attracted significant attention recently. 
  To generate tensor programs efficiently, a key component of DNN compilers is the cost model that can predict the performance of each configuration on specific devices. 
  However, due to the rapid emergence of hardware platforms, it is increasingly labor-intensive to train domain-specific predictors for every new platform. 
  Besides, current design of cost models cannot provide transferable features between different hardware accelerators efficiently and effectively. 
  In this paper, we propose Moses, a simple and efficient design based on the lottery ticket hypothesis, which fully takes advantage of the features transferable to the target device via domain adaptation. 
  Compared with state-of-the-art approaches, Moses achieves up to 1.53X efficiency gain in the search stage and 1.41X inference speedup on challenging DNN benchmarks.},
  journal={arXiv preprint arXiv:2201.05752},
  year={2022},
  url={https://arxiv.org/pdf/2201.05752.pdf},
  html={https://arxiv.org/abs/2201.05752},
  pdf={moses.pdf},
}

